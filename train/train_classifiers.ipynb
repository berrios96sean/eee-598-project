{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data = pd.read_csv('/home/rsunketa/eee-598-project/train/data/botnet_10_best_features/botnet_resampled_scaled_features.csv')\n",
    "\n",
    "# num_splits = 10\n",
    "# rows_per_split = len(data) // num_splits\n",
    "\n",
    "# # Split the data and save to separate CSV files\n",
    "# for i in range(num_splits):\n",
    "#     start_row = i * rows_per_split\n",
    "#     if i == num_splits - 1:\n",
    "#         end_row = len(data)\n",
    "#     else:\n",
    "#         end_row = (i + 1) * rows_per_split\n",
    "#     split_data = data.iloc[start_row:end_row]\n",
    "#     split_data.to_csv(f'/home/rsunketa/eee-598-project/train/data/botnet_10_best_features/botnet_resampled_scaled_features_part_{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Load the new datasets\n",
    "# X_new = pd.read_csv('/home/rsunketa/HML/datasets/botnet_10_best_features/botnet_resampled_scaled_features.csv')\n",
    "csv_files = [f'/home/rsunketa/eee-598-project/train/data/botnet_10_best_features/botnet_resampled_scaled_features_part_{i+1}.csv' for i in range(10)]\n",
    "X_new = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "y_new = pd.read_csv('/home/rsunketa/HML/datasets/botnet_10_best_features/botnet_resampled_binary_label.csv')\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1467218, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 4.50373154e+01  1.03786838e+01  1.59393044e+01  3.36462667e+00\n",
      "  1.38120942e+01 -1.24742397e+00  2.63092266e+01 -1.86548788e-02\n",
      " -1.79572620e+00  1.31178633e+00]\n",
      "Intercept: -18.654783557424274\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_new, y_train_new.values.ravel())\n",
    "\n",
    "\n",
    "coefficients = clf.coef_[0]\n",
    "intercept = clf.intercept_[0]\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9970\n"
     ]
    }
   ],
   "source": [
    "accuracy = clf.score(X_test_new, y_test_new)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # hls classifier\n",
    "\n",
    "\n",
    "# coefficients = np.array([\n",
    "#     45.0373153853, 10.3786838184, 15.9393044430, 3.3646266663, 13.8120942498,\n",
    "#     -1.2474239747, 26.3092266130, -0.0186548788, -1.7957262049, 1.3117863341\n",
    "# ])\n",
    "\n",
    "# intercept = -18.6547835574\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# all_decisions = []\n",
    "# all_probabilities = []\n",
    "\n",
    "# def logistic_regression(X):\n",
    "    \n",
    "#     decision = intercept + np.dot(X, coefficients)\n",
    "#     all_decisions.append(decision)\n",
    "    \n",
    "#     probability = sigmoid(decision)\n",
    "#     all_probabilities.append(probability)\n",
    "    \n",
    "#     predicted_class = (probability > 0.5).astype(int)\n",
    "    \n",
    "#     return predicted_class\n",
    "\n",
    "\n",
    "\n",
    "# y_pred = logistic_regression(X_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_probabilities_array = np.concatenate(all_probabilities)\n",
    "\n",
    "# min_prob = np.min(all_probabilities_array)\n",
    "# max_prob = np.max(all_probabilities_array)\n",
    "# mean_prob = np.mean(all_probabilities_array)\n",
    "# range_prob = max_prob - min_prob\n",
    "\n",
    "# print(f\"Min probability: {min_prob:.10f}\")\n",
    "# print(f\"Max probability: {max_prob:.10f}\")\n",
    "# print(f\"Mean probability: {mean_prob:.10f}\")\n",
    "# print(f\"Range of probabilities: {range_prob:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_decisions_array = np.concatenate(all_decisions)\n",
    "\n",
    "# min_prob = np.min(all_decisions_array)\n",
    "# max_prob = np.max(all_decisions_array)\n",
    "# mean_prob = np.mean(all_decisions_array)\n",
    "# range_prob = max_prob - min_prob\n",
    "\n",
    "# print(f\"Min decisions: {min_prob:.10f}\")\n",
    "# print(f\"Max decisions: {max_prob:.10f}\")\n",
    "# print(f\"Mean decisions: {mean_prob:.10f}\")\n",
    "# print(f\"Range of decisions: {range_prob:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# accuracy = accuracy_score(y_test_new, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train_new, y_train_new.values.ravel())\n",
    "\n",
    "weights_input_hidden = mlp.coefs_[0]\n",
    "biases_hidden = mlp.intercepts_[0]\n",
    "weights_hidden_output = mlp.coefs_[1]\n",
    "biases_output = mlp.intercepts_[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "y_pred_mlp = mlp.predict(X_test_new)\n",
    "\n",
    "accuracy_mlp = accuracy_score(y_test_new, y_pred_mlp)\n",
    "print(f\"Accuracy: {accuracy_mlp:.4f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "for sample in X_test_new.values:\n",
    "    mlp.predict([sample])\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "time_per_sample = (total_time / len(X_test_new)) * 1e9 \n",
    "print(f\"Average time per sample inference: {time_per_sample:.2f} ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the class labels for the test data and print ranges of intermediate values\n",
    "# hidden_activations = []\n",
    "# hidden_1_activations = []\n",
    "# output_activations = []\n",
    "# output_1_activations = []\n",
    "# predicted_classes = []\n",
    "\n",
    "# for X in X_test:\n",
    "#     hidden, output, predicted_class, hidden_1, output_1 = mlp(X)\n",
    "#     hidden_activations.append(hidden)\n",
    "#     output_activations.append(output)\n",
    "#     hidden_1_activations.append(hidden_1)\n",
    "#     output_1_activations.append(output_1)\n",
    "#     predicted_classes.append(predicted_class)\n",
    "\n",
    "\n",
    "# hidden_activations = np.array(hidden_activations)\n",
    "# output_activations = np.array(output_activations)\n",
    "# hidden_1_activations = np.array(hidden_1_activations)\n",
    "# output_1_activations = np.array(output_1_activations)\n",
    "# predicted_classes = np.array(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(f\"Range of hidden activations: {hidden_activations.min()} to {hidden_activations.max()}\")\n",
    "# print(f\"Range of output activations: {output_activations.min()} to {output_activations.max()}\")\n",
    "# print(f\"Range of hidden_1 activations: {hidden_1_activations.min()} to {hidden_1_activations.max()}\")\n",
    "# print(f\"Range of output_1 activations: {output_1_activations.min()} to {output_1_activations.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy = accuracy_score(y_test_new, predicted_classes)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0:\n",
      "Node: feature = 6, threshold = 0.2373737395, left child = 1, right child = 8\n",
      "Node: feature = 0, threshold = 0.1286235154, left child = 2, right child = 5\n",
      "Node: feature = 1, threshold = 0.6152042150, left child = 3, right child = 4\n",
      "Leaf node: value = -1.9856512911\n",
      "Leaf node: value = -0.2497286334\n",
      "Node: feature = 0, threshold = 0.5019526482, left child = 6, right child = 7\n",
      "Leaf node: value = 1.9996708597\n",
      "Leaf node: value = 1.9996708597\n",
      "Node: feature = 0, threshold = 0.0000286029, left child = 9, right child = 12\n",
      "Node: feature = 2, threshold = 0.0757575762, left child = 10, right child = 11\n",
      "Leaf node: value = -1.9986242187\n",
      "Leaf node: value = 1.9996708597\n",
      "Node: feature = 0, threshold = 0.0000324166, left child = 13, right child = 14\n",
      "Leaf node: value = 1.7496708530\n",
      "Leaf node: value = 1.9996708597\n",
      "Tree 1:\n",
      "Node: feature = 6, threshold = 0.2373737395, left child = 1, right child = 8\n",
      "Node: feature = 0, threshold = 0.1286235154, left child = 2, right child = 5\n",
      "Node: feature = 1, threshold = 0.7608265281, left child = 3, right child = 4\n",
      "Leaf node: value = -1.8039224165\n",
      "Leaf node: value = 2.0249498339\n",
      "Node: feature = 0, threshold = 0.5482760072, left child = 6, right child = 7\n",
      "Leaf node: value = 1.8184882151\n",
      "Leaf node: value = 1.8184882151\n",
      "Node: feature = 0, threshold = 0.0000286029, left child = 9, right child = 12\n",
      "Node: feature = 2, threshold = 0.0757575762, left child = 10, right child = 11\n",
      "Leaf node: value = -1.8173909463\n",
      "Leaf node: value = 1.8184882151\n",
      "Node: feature = 0, threshold = 0.0000324166, left child = 13, right child = 14\n",
      "Leaf node: value = 1.5872828713\n",
      "Leaf node: value = 1.8184882151\n",
      "Tree 2:\n",
      "Node: feature = 6, threshold = 0.2373737395, left child = 1, right child = 8\n",
      "Node: feature = 0, threshold = 0.1286235154, left child = 2, right child = 5\n",
      "Node: feature = 1, threshold = 0.5733407438, left child = 3, right child = 4\n",
      "Leaf node: value = -1.6698755212\n",
      "Leaf node: value = -0.2364409536\n",
      "Node: feature = 5, threshold = 0.0015024443, left child = 6, right child = 7\n",
      "Leaf node: value = 1.6823960290\n",
      "Leaf node: value = 1.6823960290\n",
      "Node: feature = 0, threshold = 0.0000286029, left child = 9, right child = 12\n",
      "Node: feature = 2, threshold = 0.0757575762, left child = 10, right child = 11\n",
      "Leaf node: value = -1.6812241220\n",
      "Leaf node: value = 1.6823960290\n",
      "Node: feature = 0, threshold = 0.0000324166, left child = 13, right child = 14\n",
      "Leaf node: value = 1.4589978385\n",
      "Leaf node: value = 1.6823960290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Train the Gradient Boosting classifier\n",
    "clf = GradientBoostingClassifier(n_estimators=3, max_depth=3, random_state=42)\n",
    "clf.fit(X_train_new, y_train_new.values.ravel())\n",
    "\n",
    "# Directly print tree parameters\n",
    "for i, tree in enumerate(clf.estimators_):\n",
    "    print(f\"Tree {i}:\")\n",
    "    tree = tree[0].tree_\n",
    "    for node in range(tree.node_count):\n",
    "        if tree.children_left[node] == -1:  # Leaf node\n",
    "            print(f\"Leaf node: value = {tree.value[node][0][0]:.10f}\")\n",
    "        else:\n",
    "            print(f\"Node: feature = {tree.feature[node]}, threshold = {tree.threshold[node]:.10f}, left child = {tree.children_left[node]}, right child = {tree.children_right[node]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Accuracy: 0.9980\n"
     ]
    }
   ],
   "source": [
    "y_pred_gb = clf.predict(X_test_new)\n",
    "\n",
    "accuracy_gb = accuracy_score(y_test_new, y_pred_gb)\n",
    "print(f\"Gradient Boosting Classifier Accuracy: {accuracy_gb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsunketa/.venv/lib64/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      5\u001b[0m clf \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(\n\u001b[1;32m      6\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mDecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      7\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      8\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m y_pred_ada \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test_new[:\u001b[38;5;241m100\u001b[39m])\n\u001b[1;32m     14\u001b[0m accuracy_ada \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_new[:\u001b[38;5;241m100\u001b[39m], y_pred_ada)\n",
      "File \u001b[0;32m~/.venv/lib64/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib64/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:169\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    166\u001b[0m sample_weight[zero_weight_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venv/lib64/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:594\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001b[0;32m~/.venv/lib64/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:605\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[1;32m    603\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m--> 605\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m y_predict_proba \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.venv/lib64/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib64/python3.11/site-packages/sklearn/tree/_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.venv/lib64/python3.11/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=5,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train_new, y_train_new.values.ravel())\n",
    "\n",
    "y_pred_ada = clf.predict(X_test_new[:100])\n",
    "\n",
    "accuracy_ada = accuracy_score(y_test_new[:100], y_pred_ada)\n",
    "print(f\"AdaBoost Classifier Accuracy on first 100 samples: {accuracy_ada:.4f}\")\n",
    "\n",
    "# Directly print stump parameters\n",
    "for i, estimator in enumerate(clf.estimators_):\n",
    "    tree = estimator.tree_\n",
    "    feature = tree.feature[0]\n",
    "    threshold = tree.threshold[0]\n",
    "    weight = clf.estimator_weights_[i]\n",
    "    polarity = 1 if tree.children_left[0] == 1 else -1\n",
    "    print(f\"Stump {i}: feature = {feature}, threshold = {threshold:.10f}, weight = {weight:.10f}, polarity = {polarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vectors:\n",
      "[[1.26302863e-01 6.22041202e-01 3.03030303e-02 ... 8.40667304e-06\n",
      "  5.13529341e-07 7.42969499e-01]\n",
      " [1.26794833e-01 6.72616863e-01 2.02020202e-02 ... 9.09018571e-06\n",
      "  5.55282340e-07 8.06461538e-01]\n",
      " [1.20914073e-01 0.00000000e+00 1.61616162e-01 ... 0.00000000e+00\n",
      "  3.99819609e-07 0.00000000e+00]\n",
      " ...\n",
      " [4.41705344e-02 0.00000000e+00 4.14141414e-01 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [8.51489831e-02 7.94989352e-03 1.91919192e-01 ... 0.00000000e+00\n",
      "  5.33480000e-08 9.16800183e-03]\n",
      " [4.05284294e-02 4.44924088e-02 2.72727273e-01 ... 5.57260020e-07\n",
      "  1.10357000e-07 5.70634114e-02]]\n",
      "Coefficients:\n",
      "[-1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -0.08443239 -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -0.12690702 -0.72218663 -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -0.76630876 -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -0.30802749 -1.         -1.         -0.22414769 -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.         -1.         -1.         -1.         -1.         -1.\n",
      " -1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          0.77241346\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          0.86824524  1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          0.97129368  1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          0.6200576   1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.        ]\n",
      "Intercept:\n",
      "-3.873739540016655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train_sample = X_train_new[:50000]\n",
    "y_train_sample = y_train_new[:50000]\n",
    "\n",
    "svm_clf = SVC(kernel='linear', C=1.0)\n",
    "svm_clf.fit(X_train_sample, y_train_sample.values.ravel())\n",
    "\n",
    "support_vectors = svm_clf.support_vectors_\n",
    "coefficients = svm_clf.dual_coef_[0]\n",
    "intercept = svm_clf.intercept_[0]\n",
    "\n",
    "# Directly print the parameters\n",
    "print(\"Support Vectors:\")\n",
    "print(support_vectors)\n",
    "\n",
    "print(\"Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "print(\"Intercept:\")\n",
    "print(intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(527,)\n"
     ]
    }
   ],
   "source": [
    "print(coefficients.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_100 = svm_clf.predict(X_test_new[:1000])\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test_new[:1000], y_pred_100)\n",
    "print(f\"Accuracy: {accuracy_svm:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
